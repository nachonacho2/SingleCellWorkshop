{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqFbysZdG5-8"
   },
   "source": [
    "# Exploratory data analysis in retinal bipolar data with autoencoders\n",
    "\n",
    "In this notebook, we will build a neural network that explores the retinal bipolar dataset for Shekhar et al., 2016 without using the manually annotated cell type labels.\n",
    "\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3768,
     "status": "ok",
     "timestamp": 1589307819311,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "g6ohAr1RG5-9",
    "outputId": "9da1dec2-f947-4fd6-b1ad-165ae481beec"
   },
   "outputs": [],
   "source": [
    "!pip install scprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wb_adBD-G5_B"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jiN6w4DWG5_F"
   },
   "source": [
    "## 2. Loading the retinal bipolar data\n",
    "\n",
    "We'll use the same retinal bipolar data we used for the classifier.\n",
    "\n",
    "Alternatively, you may load your own data by replacing the Google Drive file ids with your own file ids.\n",
    "\n",
    "Note that if you do, you will likely not have annotated celltype labels yet. Replace all references to `metadata['CELLTYPE']` with an entry from `metadata`, or your favorite gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAHeSG8kG5_F"
   },
   "outputs": [],
   "source": [
    "scprep.io.download.download_google_drive(\"1GYqmGgv-QY6mRTJhOCE1sHWszRGMFpnf\", \"data.pickle.gz\")\n",
    "scprep.io.download.download_google_drive(\"1q1N1s044FGWzYnQEoYMDJOjdWPm_Uone\", \"metadata.pickle.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6tKsEqhG5_I"
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle(\"data.pickle.gz\")\n",
    "metadata = pd.read_pickle(\"metadata.pickle.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctxcdSusG5_M"
   },
   "outputs": [],
   "source": [
    "data = scprep.reduce.pca(data_raw, n_components=100, method='dense').to_numpy()\n",
    "labels, cluster_names = pd.factorize(metadata['CELLTYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFBruQU6G5_P"
   },
   "source": [
    "## 3. Building an autoencoder\n",
    "\n",
    "An **autoencoder** is a network that tries to reproduce its input. \n",
    "\n",
    "In this case, we will squeeze the data through a two-dimensional bottleneck (i.e. a extremely low-dimensional hidden layer) which we can use for visualization. Also, reducing the dimension from 100 down to 2 forces the network to only retain the most important information, which intrinsically behaves as a kind of denoising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHgcTx_DG5_P"
   },
   "source": [
    "#### Create layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGwJ5J62G5_Q"
   },
   "outputs": [],
   "source": [
    "class layer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation=None):\n",
    "        super(layer, self).__init__()\n",
    "\n",
    "        self.weight = torch.randn(input_size, output_size).double().requires_grad_()\n",
    "        self.bias = torch.randn(output_size).double().requires_grad_()\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.matmul(x, self.weight) + self.bias\n",
    "        output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DiIXJ0yIar_"
   },
   "outputs": [],
   "source": [
    "# move data to pytorch tensors\n",
    "data_tensor = torch.Tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZ_vcy7_JYoR"
   },
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNh0h_88G5_W"
   },
   "outputs": [],
   "source": [
    "# layers will be input -> 100 -> 2 --> 100 -> output\n",
    "# first hidden layer of size 100\n",
    "hidden_layer1 = layer(input_size=data_tensor.shape[1], \n",
    "                      output_size= 100, \n",
    "                      activation=nn.ReLU())\n",
    "\n",
    "# we won't apply a nonlinear activation to the 2D middle layer\n",
    "hidden_layer2 = layer(input_size=100, \n",
    "                      output_size=2,\n",
    "                      activation=None)\n",
    "\n",
    "# last hidden layer of size 100\n",
    "hidden_layer3 = layer(input_size=2,\n",
    "                         output_size=100, \n",
    "                         activation=nn.ReLU())\n",
    "\n",
    "# the output should be the same size as the input\n",
    "output = layer(input_size=100,\n",
    "              output_size=data_tensor.shape[1], \n",
    "               activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a2Gmv26GJcNI"
   },
   "source": [
    "### Method 2\n",
    "\n",
    "PyTorch provides the linear layers we've been manually defining in its `nn` module (the same place we've been getting our activation functions) as [`nn.Linear()`](https://pytorch.org/docs/stable/nn.html#linear), so let's go ahead and repeat the layer creation step above using this new knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qp-ECo0UIXiF"
   },
   "outputs": [],
   "source": [
    "# layers will be input -> 100 -> 2 --> 100 -> output\n",
    "\n",
    "# first hidden layer of size 100\n",
    "hidden_layer1 = nn.Linear(in_features=data_tensor.shape[1], \n",
    "                      out_features= 100)\n",
    "\n",
    "# second middle layer\n",
    "hidden_layer2 = nn.Linear(in_features=100, \n",
    "                      out_features=2)\n",
    "\n",
    "# last hidden layer of size 100\n",
    "hidden_layer3 = nn.Linear(in_features=2,\n",
    "                        out_features=100)\n",
    "\n",
    "# the output should be the same size as the input\n",
    "output_layer4 = nn.Linear(in_features=100,\n",
    "              out_features=data_tensor.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TMArhnJvK0Dt"
   },
   "source": [
    "As you may have noticed, we did not specify our activation functions this time. Since this is separate from the `nn.Linear` class, we will have to define them outside our layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yh9oGtknLC4V"
   },
   "outputs": [],
   "source": [
    "activation_1 = nn.ReLU()\n",
    "activation_3 = nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pFE27nrLDAY"
   },
   "source": [
    "Now let's use some PyTorch magic and create a model using `nn.Sequential`, which we can just treat as some fancy list for Pytorch layers. One of the benefits of this is that we can use `model.parameters()` to pull out the list of network parameters, rather than having to list them ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GWRXvrBGLDHN"
   },
   "outputs": [],
   "source": [
    "autoencoder1 = nn.Sequential(hidden_layer1,\n",
    "                            activation_1,\n",
    "                            hidden_layer2,\n",
    "                            hidden_layer3,\n",
    "                            activation_3,\n",
    "                            output_layer4\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLUddGxqLiKU"
   },
   "source": [
    "`nn.Sequential` ties together our layers and creates a model. The data passes through the model in the order we place the layers. We can print out the model to see the list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1589309883644,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "6cNbHZeiLh9n",
    "outputId": "ce219f6e-0361-4df7-c284-f7e5f20103ed"
   },
   "outputs": [],
   "source": [
    "print(autoencoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_xDLQPMMOsB"
   },
   "source": [
    "#### Define Optimizer\n",
    "\n",
    "As in the classifier, we'll start with a SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GUS2IrrBMO0m"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = optim.SGD(autoencoder1.parameters(),\n",
    "                       lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYRAFMyaMbP4"
   },
   "source": [
    "#### Loss function\n",
    "\n",
    "Since this is an autoencoder, we don't have prior assumptions on the output (like it being a discrete probability distribution, as it was in classification) so we can't use fancy loss functions like the cross entropy. Instead, we'll just compute the mean squared error of the output compared to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpmhlP7UMbUa"
   },
   "outputs": [],
   "source": [
    "loss_fcn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9Wv8Ro_G5_a"
   },
   "source": [
    "#### Train the network\n",
    "\n",
    "Let's move our hyperparameters to a function that we can reuse to train other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gf8gVvtG5_a"
   },
   "outputs": [],
   "source": [
    "def train_model(model, n_epochs=10):\n",
    "\n",
    "    batch_size=100\n",
    "    learning_rate = 0.001\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                        lr=learning_rate)\n",
    "    loss_fcn = nn.MSELoss()\n",
    "\n",
    "    # we'll train the network for 10 epochs\n",
    "    step = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        # randomize the order of the data each time through\n",
    "        random_order = np.random.permutation(data_tensor.shape[0])\n",
    "        data_randomized = data_tensor[random_order]\n",
    "\n",
    "        # train the network on batches of size `batch_size`\n",
    "        for data_batch in np.array_split(data_randomized, data_randomized.shape[0] // batch_size):\n",
    "            step += 1\n",
    "\n",
    "            # update the network weights to minimize the loss\n",
    "            output = model(data_batch)\n",
    "\n",
    "            # get loss\n",
    "            loss = loss_fcn(output, data_batch)\n",
    "\n",
    "            # print the loss every 100 epochs\n",
    "            if step % 100 == 0:\n",
    "                print(\"Step: {} Loss: {:.3f}\".format(step, loss.item()))\n",
    "\n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7294,
     "status": "ok",
     "timestamp": 1589309967508,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "78NXZKspPxSi",
    "outputId": "2ce4cbe9-85f1-457c-e551-d80ba763dc3e"
   },
   "outputs": [],
   "source": [
    "autoencoder1 = train_model(autoencoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3aobukeG5_e"
   },
   "source": [
    "#### Visualize the output\n",
    "\n",
    "Rather than evaluating our model with our data like we did with the classifier, we can now use our model to evaluate our data (aka exploratory data analysis)!  Autoencoder networks are very useful in exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1589309975541,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "c0e1VY4WNh9n",
    "outputId": "19cd7eaa-9826-413d-89d5-74cc80beac52"
   },
   "outputs": [],
   "source": [
    "print(autoencoder1[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1864,
     "status": "ok",
     "timestamp": 1589309977029,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "waOKhh07G5_e",
    "outputId": "d2b41cd0-bce6-45c4-951b-1f856e6e12d3"
   },
   "outputs": [],
   "source": [
    "# let's get the 2D internal hidden layer and visualize it with a scatter plot\n",
    "\n",
    "with torch.no_grad():\n",
    "    ae_coordinates = autoencoder1[:3](data_tensor).numpy()\n",
    "\n",
    "\n",
    "scprep.plot.scatter2d(ae_coordinates, c=cluster_names[labels],\n",
    "                      label_prefix='AE Coordinate ', discrete=True,\n",
    "                      legend_anchor=(1,1), figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try retraining the network for more than just 10 epochs and plot it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======\n",
    "# Retrain the network\n",
    "autoencoder1 = train_model(\n",
    "    autoencoder1,\n",
    "    n_epochs =\n",
    ")\n",
    "# ======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ae_coordinates = autoencoder1[:3](data_tensor).numpy()\n",
    "\n",
    "scprep.plot.scatter2d(ae_coordinates, c=cluster_names[labels],\n",
    "                      label_prefix='AE Coordinate ', discrete=True,\n",
    "                      legend_anchor=(1,1), figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1qISZNPEG5_h"
   },
   "source": [
    "### Discussion\n",
    "\n",
    "1. What do you notice about the visualization? \n",
    "2. How does this compare to the visualizations you have seen with PCA, t-SNE, UMAP and PHATE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzYwYLfSG5_i"
   },
   "source": [
    "## Exercise 4 - Activation functions on the visualization layer\n",
    "\n",
    "Notice we did not use an acitvation function for the hidden layer we were going to visualize.\n",
    "\n",
    "Repeat the process with other activation functions like `nn.ReLU`, `nn.Sigmoid`, `nn.Tanh`, etc. You can see more in the [PyTorch activation function documentions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity). \n",
    "\n",
    "Note how the visualization changes. Has the data changed at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcbCs6mbPKpP"
   },
   "outputs": [],
   "source": [
    "# ======\n",
    "# choose from nn.Sigmoid(), nn.Tanh() and others in the documentation\n",
    "activation_2 = \n",
    "# ======\n",
    "\n",
    "autoencoder2 = nn.Sequential(hidden_layer1,\n",
    "                            activation_1,\n",
    "                            hidden_layer2,\n",
    "                            activation_2,\n",
    "                            hidden_layer3,\n",
    "                            activation_3,\n",
    "                            output_layer4\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxsUbDK_G5_i"
   },
   "outputs": [],
   "source": [
    "autoencoder2 = train_model(autoencoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2URGbD5G5_m"
   },
   "outputs": [],
   "source": [
    "# let's get the 2D internal hidden layer and visualize it with a scatter plot\n",
    "\n",
    "with torch.no_grad():\n",
    "    ae_coordinates2 = autoencoder2[:4](data_tensor).numpy()\n",
    "\n",
    "scprep.plot.scatter2d(ae_coordinates2, c=cluster_names[labels],\n",
    "                      label_prefix='AE Coordinate ', discrete=True,\n",
    "                      legend_anchor=(1,1), figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBJbtz5QG5_q"
   },
   "source": [
    "## Exercise 5 - Activation functions on the wide hidden layers\n",
    "\n",
    "Now turn the activation for the visualization layer back to None, but experiment with the activation function for the 100-dimensional layers.\n",
    "\n",
    "Is there a change? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7feryPNnG5_q"
   },
   "outputs": [],
   "source": [
    "# ======\n",
    "# choose from nn.Sigmoid(), nn.Tanh() and others in the documentation\n",
    "activation_1 = \n",
    "activation_3 = \n",
    "# ======\n",
    "\n",
    "autoencoder3 = nn.Sequential(hidden_layer1,\n",
    "                            activation_1,\n",
    "                            hidden_layer2,\n",
    "                            hidden_layer3,\n",
    "                            activation_3,\n",
    "                            output_layer4\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder3 = train_model(autoencoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4JMhkqt3G5_u"
   },
   "outputs": [],
   "source": [
    "# let's get the 2D internal hidden layer and visualize it with a scatter plot\n",
    "\n",
    "with torch.no_grad():\n",
    "    ae_coordinates3 = autoencoder3[:3](data_tensor).numpy()\n",
    "\n",
    "scprep.plot.scatter2d(ae_coordinates3, c=cluster_names[labels],\n",
    "                      label_prefix='AE Coordinate ', discrete=True,\n",
    "                      legend_anchor=(1,1), figsize=(10,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
