{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scprep graphtools louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rc('font', size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import sklearn.datasets\n",
    "\n",
    "import scprep\n",
    "import louvain\n",
    "import graphtools as gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of clustering algorithms on toy data\n",
    "\n",
    "Here we're going to compare three clustering algorithms on toy data. The code for this exercise is adapted from: https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data\n",
    "\n",
    "Here we're going to work with 6 datasets all in $\\mathbb{R}^2$ \n",
    "\n",
    "1. **Circles** - two circles, one circumscribed by the other\n",
    "2. **Moons** - Two interleaving half circles\n",
    "3. **Varied variance blobs** - These blobs each have different variances\n",
    "4. **Anisotropically distributed blobs** - these blobs have unequal widths and lengths\n",
    "5. **Regular blobs** - Just three regular blobs\n",
    "6. **Uniformly sampled square** - Just a single square\n",
    "\n",
    "Because we're generating these from scratch, we get to change some parameters of their distributions. Generally, we can change:\n",
    "1. `noise` - the amount of Gaussian noise added to each point\n",
    "2. `n_samples` - the number of points generated\n",
    "3. `factor` / `cluster_std` - some parameters affecting shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "\n",
    "# Circles\n",
    "noisy_circles = sklearn.datasets.make_circles(\n",
    "    n_samples=n_samples, \n",
    "    # Scale factor between inner and outer circle\n",
    "    factor=.5,\n",
    "    # Gaussian noise added to each point\n",
    "    noise=.05)\n",
    "\n",
    "# Moons\n",
    "noisy_moons = sklearn.datasets.make_moons(n_samples=n_samples, \n",
    "                                          noise=.05)\n",
    "\n",
    "# Blobs\n",
    "blobs = sklearn.datasets.make_blobs(n_samples=n_samples, random_state=8, cluster_std=1)\n",
    "\n",
    "# Uniform square\n",
    "no_structure = (np.random.uniform(size=(n_samples, 2)), None)\n",
    "\n",
    "# Anisotropically distributed data\n",
    "random_state = 170\n",
    "X, y = sklearn.datasets.make_blobs(n_samples=n_samples, random_state=random_state, cluster_std=1)\n",
    "# Changes how x1, x2 coordinates are shifted\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = sklearn.datasets.make_blobs(n_samples=n_samples,\n",
    "                                     cluster_std=[1.0, 2.5, 0.5],\n",
    "                                     random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Associate each dataset with the correct # of clusters\n",
    "# ============\n",
    "\n",
    "default_base = {'n_clusters': 3}\n",
    "\n",
    "generated_datasets = [\n",
    "    (noisy_circles, {'n_clusters': 2}),\n",
    "    (noisy_moons, {'n_clusters': 2}),\n",
    "    (varied,      {}),\n",
    "    (aniso,       {}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ground truth cluster assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,6,figsize=(12,2))\n",
    "\n",
    "for i, (dataset, _) in enumerate(generated_datasets):\n",
    "    ax = axes[i]\n",
    "    X, y = dataset\n",
    "    \n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = sklearn.preprocessing.StandardScaler().fit_transform(X)\n",
    "    scprep.plot.scatter2d(X, c=y, \n",
    "                          ticks=None, ax=ax, \n",
    "                          xlabel='x0', ylabel='x1',\n",
    "                         legend=False)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run clustering algorithms and plot results\n",
    "\n",
    "This is a lot of code, so make sure you and your group go through and understand what's going on. Ask for help if you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6,3, figsize=(12, 20))\n",
    "plot_title = True\n",
    "\n",
    "for i_dataset, (dataset, cluster_params) in enumerate(generated_datasets):\n",
    "    # update cluster parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(cluster_params)\n",
    "\n",
    "    X, y = dataset\n",
    "    \n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = sklearn.preprocessing.StandardScaler().fit_transform(X)\n",
    "    \n",
    "        \n",
    "    # ============\n",
    "    # Run clustering algorithms\n",
    "    # ============\n",
    "    clusters = []\n",
    "    titles = []\n",
    "    times = []\n",
    "    # KMeans\n",
    "    tic = time.time()\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=params['n_clusters'])\n",
    "    clusters.append(kmeans.fit_predict(X))\n",
    "    titles.append('KMeans')\n",
    "    times.append(time.time() - tic)\n",
    "    \n",
    "    # Spectral Clustering\n",
    "    tic = time.time()\n",
    "    spectral = sklearn.cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    clusters.append(spectral.fit_predict(X))\n",
    "    titles.append('Spectral')\n",
    "    times.append(time.time() - tic)\n",
    "\n",
    "    \n",
    "    # Louvain\n",
    "    tic = time.time()\n",
    "    G = gt.Graph(X)\n",
    "    G_igraph = G.to_igraph()\n",
    "    part = louvain.find_partition(G_igraph, louvain.RBConfigurationVertexPartition, \n",
    "                                  weights=\"weight\", resolution_parameter=0.01)\n",
    "    clusters.append(np.array(part.membership))\n",
    "    titles.append('Louvain')\n",
    "    times.append(time.time() - tic)\n",
    "\n",
    "    # ============\n",
    "    # Plot clustering results for dataset\n",
    "    # ============\n",
    "    row_axes = axes[i_dataset]\n",
    "    \n",
    "    for i, ax in enumerate(row_axes.flatten()):\n",
    "        curr_cluster = clusters[i]\n",
    "        if plot_title:\n",
    "            curr_title = '{}'.format(titles[i])\n",
    "        else:\n",
    "            curr_title = None\n",
    "            \n",
    "        scprep.plot.scatter2d(X, c=curr_cluster, title=curr_title, ax=ax,\n",
    "                             legend=False, discrete=True)\n",
    "\n",
    "        # Plot time to run algorithm\n",
    "        plt.text(.99, .01, ('%.2fs' % (times[i])).lstrip('0'),\n",
    "                 transform=ax.transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "    plot_title=False\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "In groups, change the following features for one or more of the distributions\n",
    "1. `noise` - the amount of Gaussian noise added to each point\n",
    "2. `n_samples` - the number of points generated\n",
    "3. `factor` / `cluster_std` / `transformation` - some parameters affecting shape\n",
    "\n",
    "Try to identify:\n",
    "1. A set of parameters that makes `SpectralClustering` fail on the circles dataset\n",
    "2. `Louvain` fail on the anisotropically distributed blobs\n",
    "3. `KMeans` fail on the three regular blobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the correct number of clusters\n",
    "\n",
    "Although it is possible to visually inspect the data and select an ideal number of clusters, there are several measures for quantitatively selecting the ideal number of partitions for a given dataset. Here, we're going to focus on one measure, called the Silhouette score, which calculates a ratio between the inter- and intra- distances for a given set of cluster assignments. We'll describe that more below.\n",
    "\n",
    "For now, we're going to generate scRNA-seq data with a known ground-truth using Splatter, [Zappia L, et al. Genome Biology. 2017](https://doi.org/10.1186/s13059-017-1305-0). Splatter is a tool for simulating single-cell RNA-sequencing data with a known topology.\n",
    "\n",
    "First, let's download the necessary packages. If you were running this without precompiled packages, you could install Splatter by running the following code:\n",
    "\n",
    "```python\n",
    "import scprep\n",
    "scprep.run.splatter.install()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scprep.io.download.download_google_drive(\"1IzoUkPcw4yw0oTB9IesoPZOZIUAjONlP\", 'r_packages.tar.gz')\n",
    "!tar xzf r_packages.tar.gz  -C /usr/local/lib/R/site-library/ && rm r_packages.tar.gz\n",
    "!apt-get install -yqq libgsl-dev=2.4+dfsg-6\n",
    "!pip install --upgrade rpy2\n",
    "!R -e \"BiocManager::install('splatter')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splatter has two main modes: \"paths\" and \"groups.\" Because we're interested in generating data with a \"true\" number of clusters, we're going to use the **groups** mode. Splatter is implemented in R, but we can run it using `scprep.run.SplatSimulate`. Splatter has a ton of parameters, and you can find all of them here: https://scprep.readthedocs.io/en/stable/reference.html#scprep.run.SplatSimulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Splatter\n",
    "import scprep\n",
    "\n",
    "# You can easily change the number of clusters, cells, \n",
    "# and the distance between clusters here\n",
    "n_clusters = 15\n",
    "n_cells = 2000\n",
    "differential_expression_factor = 0.5\n",
    "\n",
    "# Getting cluster probabilities to sum to 1\n",
    "cluster_probabilities = np.random.uniform(size=n_clusters)\n",
    "cluster_probabilities = cluster_probabilities / np.sum(cluster_probabilities)\n",
    "\n",
    "# Splatter returns a `dict` objbect that contains a bunch of useful information\n",
    "results = scprep.run.SplatSimulate(\n",
    "                        method='groups', \n",
    "                        batch_cells=n_cells, \n",
    "                        group_prob=cluster_probabilities, \n",
    "                        n_genes=5000,\n",
    "                        de_fac_loc=differential_expression_factor,\n",
    "                        seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put counts data in DataFrame\n",
    "data = pd.DataFrame(results['counts'])\n",
    "\n",
    "# Put metadata in a DataFrame\n",
    "metadata = pd.DataFrame({'group':results['group']})\n",
    "\n",
    "# clean up group labels from e.g. Group3 to 3\n",
    "metadata['group'] = metadata['group'].str[5:].astype(int)\n",
    "\n",
    "# Change DataFrame index names to be a little nicer\n",
    "new_index = pd.Index(['cell_{}'.format(i) for i in range(metadata.shape[0])])\n",
    "data.index = new_index\n",
    "metadata.index = new_index\n",
    "\n",
    "# Library-size normalize and sqrt transform\n",
    "data = scprep.normalize.library_size_normalize(data)\n",
    "data = scprep.transform.sqrt(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = scprep.reduce.pca(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scprep.plot.scatter2d(data_pca, c=metadata['group'], legend_anchor=(1,1), figsize=(8.6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a bar graph showing the number of cells per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, counts = np.unique(metadata['group'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for g, c in zip(groups, counts):\n",
    "    ax.bar(g,c)\n",
    "ax.set_xticks(groups)\n",
    "ax.set_xlabel('Ground Truth Groups')\n",
    "ax.set_ylabel('Cells in Group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Silhouette Score\n",
    "\n",
    "The silhouette is a measure of fit for a given set of cluster assignments and dataset. The silhouette score calculates the ratio between the average intra-cluster distance and inter-cluster distances. This score is often calculated over a various numbers of clusters, and the maximum is chosen for clustering. Values close to 1 are good.  Values close to -1 are bad. This measure is calculated over different `n_clusters` values, and the setting with the highest score is the recommended number of clusters.\n",
    "\n",
    "\n",
    "\n",
    "This measure is really only useful for methods where `n_clusters` is set by the user, so we'll focus on `KMeans` and `SpectralClustering` for this section.\n",
    "\n",
    "More information on this score can be found in the `sklearn` user guide: https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering with `KMeans` \n",
    "\n",
    "The silhouette score is most often used with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans\n",
    "kmeans_clusters = {}\n",
    "cluster_counts = np.arange(2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters in cluster_counts:\n",
    "      kmeans_clusters[n_clusters] = sklearn.cluster.KMeans(n_clusters=n_clusters).fit_predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for n_clusters in cluster_counts:\n",
    "    scores.append(sklearn.metrics.silhouette_score(data, kmeans_clusters[n_clusters] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = scprep.plot.scatter(cluster_counts, scores,\n",
    "                         xlabel='n_clusters', ylabel='silhouette score')\n",
    "\n",
    "# Plot the largest value in red\n",
    "best_score_idx = np.argmax(scores)\n",
    "best_n_clusters = cluster_counts[best_score_idx]\n",
    "scprep.plot.scatter(best_n_clusters, scores[best_score_idx], c='red', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Plotting the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = kmeans_clusters[3]\n",
    "\n",
    "scprep.plot.scatter2d(data_pca, c=clusters,\n",
    "                      ticks=None, title='PCA', \n",
    "                      label_prefix='PC',\n",
    "                      figsize=(5,5.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "In groups, try adjusting:\n",
    "1. The number of clusters generated  \n",
    "2. The distance between each cluster \n",
    "3. The clustering algorithm from `KMeans` to `SpectralClustering` (in the code block where `sklearn.cluster.KMeans` is called)\n",
    "\n",
    "Identify:\n",
    "1. A set of parameters where the silhouette score perfectly indicates the correct number of clusters\n",
    "2. A set of parameters where the silhouette score fails to indicate the correct number of clusters\n",
    "3. Does `KMeans` or `SpectralClustering` produce better silhouette scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering trajectories\n",
    "\n",
    "Clustering is all about partitioning data. That means that even if data is continuous, you might want to use clustering to differentiate between different regions of the data. Here, the idea of a true number of clusters breaks down because observations are not distributed as a centroid + noise. \n",
    "\n",
    "However, it is not objectionable to think that there are \"good\" clusterings of a dataset and \"less good\" ones. For example, if two branches of a dataset are grouped together, that's less than ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating data with Splatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Splatter\n",
    "\n",
    "# You can easily change the number of paths and cells here\n",
    "n_paths = 3\n",
    "cells_per_path = 1000\n",
    "differential_expression_factor = 0.5\n",
    "\n",
    "# Splatter returns a `dict` objbect that contains a bunch of useful information\n",
    "results = scprep.run.SplatSimulate(\n",
    "                        method='paths', \n",
    "                        batch_cells=cells_per_path * n_paths, \n",
    "                        group_prob=np.tile(1/n_paths, n_paths), \n",
    "                        n_genes=5000,\n",
    "                        de_down_prob=0.5,\n",
    "                        de_fac_loc=differential_expression_factor,\n",
    "                        path_from=[0]*n_paths,\n",
    "                        mean_shape=1,\n",
    "                        seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing simulated data\n",
    "\n",
    "Since this simulation is designed to emulate scRNAseq, we can preprocess it in much the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put counts data in DataFrame\n",
    "data = pd.DataFrame(results['counts'])\n",
    "\n",
    "# Put metadata in a DataFrame\n",
    "metadata = pd.DataFrame({'group':results['group'], 'step':results['step'].astype(int)})\n",
    "\n",
    "# clean up group labels from e.g. Path3 to 3\n",
    "metadata['group'] = metadata['group'].str[4:].astype(int)\n",
    "\n",
    "# Change DataFrame index names to be a little nicer\n",
    "new_index = pd.Index(['cell_{}'.format(i) for i in range(metadata.shape[0])])\n",
    "data.index = new_index\n",
    "metadata.index = new_index\n",
    "\n",
    "# Library-size normalize and sqrt transform\n",
    "data = scprep.normalize.library_size_normalize(data)\n",
    "data = scprep.transform.sqrt(data)\n",
    "\n",
    "# Computing PCA\n",
    "data_pca = scprep.reduce.pca(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the ground truth\n",
    "\n",
    "The dataset naturally exists as three branches, and is not particularly well described as disconnected clusters. However, in biology, data is frequently trajectory-like, but we still want to try to characterize groups as clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scprep.plot.scatter2d(data_pca, c=metadata['group'],\n",
    "                      ticks=None, title='PCA', \n",
    "                      label_prefix='PC',\n",
    "                      figsize=(5,5.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans\n",
    "kmeans_clusters = {}\n",
    "cluster_counts = np.arange(2, 20)\n",
    "\n",
    "for n_clusters in cluster_counts:\n",
    "      kmeans_clusters[n_clusters] = sklearn.cluster.KMeans(n_clusters=n_clusters).fit_predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for n_clusters in cluster_counts:\n",
    "    scores.append(sklearn.metrics.silhouette_score(data, kmeans_clusters[n_clusters] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = scprep.plot.scatter(cluster_counts, scores, xlabel='n_clusters', ylabel='silhouette score')\n",
    "\n",
    "# Plot the largest value in red\n",
    "best_score_idx = np.argmax(scores)\n",
    "best_n_clusters = cluster_counts[best_score_idx]\n",
    "scprep.plot.scatter(best_n_clusters, scores[best_score_idx], c='red', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Plotting the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = kmeans_clusters[3]\n",
    "\n",
    "\n",
    "scprep.plot.scatter2d(data_pca, c=clusters,\n",
    "                      ticks=None, title='PCA', \n",
    "                      label_prefix='PC',\n",
    "                      figsize=(5,5.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "In groups, try turns adjusting:\n",
    "1. The number of paths generated  \n",
    "2. The distance between each path \n",
    "3. The clustering algorithm from `KMeans` to `SpectralClustering` (in the code block where `sklearn.cluster.KMeans` is called)\n",
    "\n",
    "Identify:\n",
    "1. A set of parameters where the silhouette score perfectly indicates the correct number of clusters\n",
    "2. A set of parameters where the silhouette score fails to indicate the correct number of clusters\n",
    "3. Does `KMeans` or `SpectralClustering` produce better silhouette scores?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
