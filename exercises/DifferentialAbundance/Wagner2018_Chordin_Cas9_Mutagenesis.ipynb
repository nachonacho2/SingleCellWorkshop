{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MELD to characterize chordin loss-of-function\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate how to use MELD to characterize the effect of Cas9-mutagenesis in the zebrafish embryo. We will use a dataset generated by the Klein and Megason labs and published in [Wagner et al. (2018) (doi: 10.1126/science.aar4362)](https://www.ncbi.nlm.nih.gov/pubmed/29700229). Here, zebrafish embryos were injected with Cas9 + gRNAs at the one-cell stage targeting either chordin (*chd*) in the experimental condition or tyrosinase (*tyr*) in the control condition. Embryos were collected in a rough time course from 14-16 hours post fertilization for scRNA-seq and 27,000 cells were recovered.\n",
    "\n",
    "[Chordin](https://www.genecards.org/cgi-bin/carddisp.pl?gene=CHRD) is a BMP antagonist required for proper specification of dorsally-derived neural tissues ([Hammerschmidt et al. 1997](https://www.ncbi.nlm.nih.gov/pubmed/9007232)). [Tyrosinase](https://www.genecards.org/cgi-bin/carddisp.pl?gene=TYR) is a gene required for melanin production, but does not affect cell type specification at the time points considered in this study.\n",
    "\n",
    "We will also introduce some basics of preprocessing, visualization and imputation to give an idea of how you might include MELD in a general scRNA-seq analysis workflow.\n",
    "\n",
    "**Note:** this is a modified and abbreviated version of the original notebook available on [the MELD GitHub](https://github.com/KrishnaswamyLab/MELD). The full version includes parameter optimization and VFC on subclusters.\n",
    "\n",
    "Here's the order we'll follow:\n",
    "\n",
    "* [1. Loading the dataset](#1.-Loading-data)  \n",
    "* [2. Embedding Data Using PHATE](#3.-Embedding-Data-Using-PHATE)\n",
    "* [3. Using MELD to calculate sample-associated density estimates and likelihood](#4.-Using-MELD-to-calculate-sample-associated-density-estimates-and-relative-likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installing packages\n",
    "\n",
    "If you haven't installed MELD yet, you can do so from this notebook. We'll also install some other useful packages while we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user meld phate magic-impute cmocean diffxpy seaborn scanpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard imports**\n",
    "\n",
    "Note: if you get an error here, you may have to restart the runtime to make sure Colab recognizes the newly installed packages. Go to _Runtime_ -> _Restart runtime_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean\n",
    "import phate\n",
    "import scprep\n",
    "import meld\n",
    "import sklearn\n",
    "import tempfile\n",
    "import os\n",
    "import scanpy as sc\n",
    "\n",
    "# making sure plots & clusters are reproducible\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "To facilitate running this notebook quickly, we're going to download a preprocessed AnnData object from FigShare.\n",
    "\n",
    "The preprocessing steps to download the published data from GEO and perform filtering, library size normalization, and sqrt transformation can be found in the full tutorial on [the MELD GitHub](https://github.com/KrishnaswamyLab/MELD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://ndownloader.figshare.com/files/25687247?private_link=f194ae7d6bcec9bd11a3\"\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tempdir:\n",
    "        filepath = os.path.join(tempdir, \"Klein2018_Zebrafish.h5ad\")\n",
    "        scprep.io.download.download_url(URL, filepath)\n",
    "        adata = sc.read_h5ad(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample\n",
    "\n",
    "To enable this dataset to run in Google CoLab, we need to subsample to 10,000 cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_index = np.random.choice(adata.shape[0], size=10000, replace=False)\n",
    "adata = adata[subsample_index].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = adata.to_df()\n",
    "metadata = adata.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the number of cells in each sample past filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a colormap for visualizating the samples. You can select colors by hex code with Google's [RGB color picker](https://www.google.com/search?client=firefox-b-1-d&q=rgb+color+picker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cmap = {\n",
    "    'chdA': '#fb6a4a',\n",
    "    'chdB': '#de2d26',\n",
    "    'chdC': '#a50f15',\n",
    "    'tyrA': '#6baed6',\n",
    "    'tyrB': '#3182bd',\n",
    "    'tyrC': '#08519c'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the following plot, there are many more cells that passed QC in the chdA condition relative to the other samples. To account for this, the MELD algorithm automatically normalizes each replicate to account for varying numbers of cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "groups, counts = np.unique(metadata['sample_labels'], return_counts=True)\n",
    "for i, c in enumerate(counts):\n",
    "    ax.bar(i, c, color=sample_cmap[groups[i]])\n",
    "    \n",
    "ax.set_xticks(np.arange(i+1))\n",
    "ax.set_xticklabels(groups)\n",
    "ax.set_ylabel('# cells')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding Data Using PHATE\n",
    "\n",
    "The API of PHATE models that of Scikit Learn. First, you instantiate a PHATE estimator object with the parameters for fitting the PHATE embedding to a given dataset. Next, you use the `fit` and `fit_transform` functions to generate an embedding. For more information, check out [**the PHATE readthedocs page**](http://phate.readthedocs.io/).\n",
    "\n",
    "We'll just use the default parameters for now, but the following parameters can be tuned (read our documentation at [phate.readthedocs.io](https://phate.readthedocs.io/) to learn more):\n",
    "\n",
    "* `knn` : Number of nearest neighbors (default: 5). Increase this (e.g. to 20) if your PHATE embedding appears very disconnected. You should also consider increasing `knn` if your dataset is extremely large (e.g. >100k cells)\n",
    "* `decay` : Alpha decay (default: 15). Decreasing `decay` increases connectivity on the graph, increasing `decay` decreases connectivity. This rarely needs to be tuned. Set it to `None` for a k-nearest neighbors kernel.\n",
    "* `t` : Number of times to power the operator (default: 'auto'). This is equivalent to the amount of smoothing done to the data. It is chosen automatically by default, but you can increase it if your embedding lacks structure, or decrease it if the structure looks too compact.\n",
    "* `gamma` : Informational distance constant (default: 1). `gamma=1` gives the PHATE log potential, but other informational distances can be interesting. If most of the points seem concentrated in one section of the plot, you can try `gamma=0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the simplest way to apply PHATE:\n",
    "```python\n",
    "phateop = phate.PHATE(knn=9, decay=10, gamma=0, n_jobs=-2)\n",
    "Y = phateop.fit_transform(data_sqrt)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = scprep.reduce.pca(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phate_op = phate.PHATE(n_jobs=-1)\n",
    "data_phate = phate_op.fit_transform(data_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloring a PHATE plot by sample ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we plot using `scprep.plot.scatter2d`. For more advanced plotting, we recommend Matplotlib. If you want more help on using Matplotlib, they have [**extensive documentation**](https://matplotlib.org/tutorials/index.html) and [**many Stackoverflow threads**](https://stackoverflow.com/questions/tagged/matplotlib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scprep.plot.scatter2d(data_phate, c=metadata['sample_labels'], cmap=sample_cmap, \n",
    "                      legend_anchor=(1,1), figsize=(6,5), s=10, label_prefix='PHATE', ticks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloring a PHATE plot by ClusterIDs\n",
    "\n",
    "In Wagner et al. (2018), cells from the *chd* and *tyr* conditions were assigned cluster IDs through projection back to a reference dataset. In the published analysis, these number of cells mapping to each cluster in the *chd* vs *tyr* condition was used at the measure of *chd* loss-of-function on that cluster. To visualize the relationships between these clusters, we will color the PHATE plot by each cell's published ClusterID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scprep.plot.scatter2d(data_phate, c=metadata['cluster'], cmap=cmocean.cm.phase, \n",
    "                      legend_anchor=(1,1), figsize=(5,5), s=10, label_prefix='PHATE', ticks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Question\n",
    "\n",
    "1. What do you notice about this PHATE plot when you compare to the distribution of sample labels above? Are there some clusters that you think are more or less suited to analysis of differential abundance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using MELD to calculate sample-associated density estimates and relative likelihood\n",
    "\n",
    "Using MELD, we quantify the effect of an experimental perturbation by first estimating the density of each sample over a graph learned from all cells from all samples. This yields one density estimate per sample. We then normalize density estimates across samples from the same replicate to calculate the sample-associated relative likelihood. This relative likelihood is a ratio between the sample probability densities from each condition and indicates how much more likely we are to observe a given cell in one condition relative to another. \n",
    "\n",
    "We can use the relative likelihood estimates to identify which cells are the most enriched in each experimental condition and which cell types are unchanging across conditions. We can also use this value to identify the gene signature of a perturbation (*i.e.* the genes that change the most across experimental conditions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating replicate and conditions\n",
    "\n",
    "We run the MELD algorithm on each sample independently, then normalize within each replicate. First we're going to create a vector that indicated the replicate that each cell was sequenced in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['genotype_name'] = np.where(metadata['sample_labels'].str.startswith(\"chd\"), \"chd\", \"tyr\")\n",
    "metadata['genotype'] = np.where(metadata['genotype_name'] == \"chd\", 1, 0)\n",
    "metadata['replicate'] = metadata['sample_labels'].str[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MELD\n",
    "\n",
    "These next two code blocks build the graph for MELD and estimate the density of each sample. The parameters for knn and beta are optimized in the [full notebook on GitHub](https://github.com/KrishnaswamyLab/MELD).\n",
    "\n",
    "Here, we're going to create a MELD operator object, which inherits from the sklearn [`BaseEstimator`](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html). The full documentation for MELD can be found here: https://meld-docs.readthedocs.io/en/stable/\n",
    "\n",
    "The input to MELD is the data and the sample labels. Here, we're only using the first 100 PCs of the data. The output of `meld_op.fit_transform()` is the sample associated density estimate referenced in the [MELD paper](https://www.biorxiv.org/content/10.1101/532846v4). This is equivalent to a kernel density estimate of the sample over the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta is the amount of smoothing to do for density estimation\n",
    "# knn is the number of neighbors used to set the kernel bandwidth\n",
    "meld_op = meld.MELD(beta=67, knn=7)\n",
    "sample_densities = meld_op.fit_transform(data_pca, sample_labels=metadata['sample_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the sample densities on a PHATE plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,3, figsize=(11,6))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    density = sample_densities.iloc[:,i]\n",
    "    scprep.plot.scatter2d(data_phate, c=density,\n",
    "                          title=density.name,\n",
    "                          vmin=0, \n",
    "                          ticks=False, ax=ax)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion question\n",
    "\n",
    "1. What is the sum of the density of each sample over the data? How would you calculate this?\n",
    "\n",
    "2. What do you notice about the density of each sample across replicates? Do you see more similarity between samples of the same replicate or of the same condition? Is this expected or unexpected? Does it change your interpreation of the results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the densities, we then compare the densities between conditions within each replicate. This gives us a relative likelihood that a given cell would be observed in each condition. \n",
    "\n",
    "In common speech, likelihood and probability are used interchangeably. However they have very distinct statistical meanings. The probability (or probability density) of an event is the chance that an event will happen under a given model (probability of the data given the model). In this case, the sample densities are the probability that if you were to randomly pick a new cell from that sample that it would be a given cell. However, when comparing the densities for each sample for a specific cell, these values can be considered the likelihood that the cell would be observed in a given sample (likelihood of the model given the data). Here the configuration of experimental variables is considered the \"model parameters\" for the likelihood.\n",
    "\n",
    "To better understand this distinction, I recommend [the Likelihood Function article on Wikipedia](https://en.wikipedia.org/wiki/Likelihood_function#Discrete_probability_distribution).\n",
    "\n",
    "We want to calculate the ratio between these likelihoods so that we can understand how much more likely it would be to observe a cell in the treatment condition relative to the control condition. To calculate this ratio, we apply an L1 normalization of the densities within each replicate. This normalizes the values to sum to 1 across samples within each replicate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function to apply L1 normalization across the densities for each replicate\n",
    "def replicate_normalize_densities(sample_densities, replicate):\n",
    "    # Get the unique replicates\n",
    "    replicates = np.unique(replicate)\n",
    "    sample_likelihoods = sample_densities.copy()\n",
    "    for rep in replicates:\n",
    "        # Select the columns of `sample_densities` for that replicate\n",
    "        curr_cols = sample_densities.columns[[col.endswith(rep) for col in sample_densities.columns]]\n",
    "        curr_densities = sample_densities[curr_cols]\n",
    "        # Apply L1 normalization\n",
    "        sample_likelihoods[curr_cols] = sklearn.preprocessing.normalize(curr_densities, norm='l1')\n",
    "    return sample_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_likelihoods = replicate_normalize_densities(sample_densities, metadata['replicate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the sample associated relative likelihoods for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(13,4))\n",
    "\n",
    "experimental_samples = ['chdA', 'chdB', 'chdC']\n",
    "\n",
    "for curr_sample, ax in zip(experimental_samples, axes):\n",
    "    scprep.plot.scatter2d(data_phate, c=sample_likelihoods[curr_sample], cmap=meld.get_meld_cmap(),\n",
    "                          vmin=0, vmax=1,\n",
    "                          title=curr_sample, ticks=False, ax=ax)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the mean and standard deviation of the relative likelihood estimates across replicates. Notice how areas that are consistently enriched or depleted across replicates have low standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(8.7,4))\n",
    "\n",
    "scprep.plot.scatter2d(data_phate, c=sample_likelihoods[experimental_samples].mean(axis=1), \n",
    "                      cmap=meld.get_meld_cmap(), vmin=0, vmax=1,\n",
    "                      title='Mean', ticks=False, ax=axes[0])\n",
    "scprep.plot.scatter2d(data_phate, c=sample_likelihoods[experimental_samples].std(axis=1), vmin=0, \n",
    "                      cmap='inferno', title='St. Dev.', ticks=False, ax=axes[1])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the average likelihood of the chordin samples as the measure of the perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['chd_likelihood'] = sample_likelihoods[experimental_samples].mean(axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Questions:\n",
    "1. Here, we only look at the `chd` relative likelihood. Why don't we look at the `tyr` relative likelihood?\n",
    "\n",
    "2. What does the variation in the relative likelihood values across replicates tell you?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the distribution of _chd_ likelihood values in published clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will compare using clusters based on data geometry to using MELD for quantifying the effect of an experimental perturbation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the index of each cluster from lowest to highest average _chd_ likelihood value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['clusterID'] = scprep.utils.sort_clusters_by_values(metadata['clusterID'], metadata['chd_likelihood'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create jitter plots\n",
    "\n",
    "These show the distribution of _chd_ likelihood values within each cluster. Each point is a cell and the y-axis is the _chd_ likelihood. The slight jitter in the x-xais is  only to help show density within each cluster.\n",
    "\n",
    "In grey, `scprep` plots a circle denoting the mean likelihood value of each cluster. Additionally, we will plot a circle in purple denoting the ratio (or fold-change) of _tyr_ to _chd_ cells in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,10))\n",
    "\n",
    "# See example usage: https://scprep.readthedocs.io/en/stable/examples/jitter.html\n",
    "scprep.plot.jitter(metadata['clusterID'], metadata['chd_likelihood'], \n",
    "                   c=metadata['sample_labels'], \n",
    "                   cmap=sample_cmap,\n",
    "                   legend=False, \n",
    "                   plot_means=True, \n",
    "                   means_s=50, \n",
    "                   xlabel=False, \n",
    "                   ylabel='Mean chd likelihood',\n",
    "                   ax=ax)\n",
    "\n",
    "### This code will plot the ratio of tyr:chd cells per cluster\n",
    "means = metadata.groupby('clusterID')['genotype'].mean()\n",
    "ax.scatter(means.index, means - np.mean(metadata['genotype']) + 0.5, color='#7c5295', edgecolor='k', s=50)\n",
    "\n",
    "# Axis tick labels\n",
    "ax.set_xticklabels(pd.unique(metadata.sort_values('clusterID')['cluster']), rotation=90)\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "1. What do you notice about the distribution of relative likelihood values within each cluster? Are there clusters where the fold-change in abundance (purple circles) and average likelihood (grey circles) differ greatly? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "Here, we want to visualize the heterogeneity within some clusters using PHATE run on each subset of the data.\n",
    "\n",
    "Pick one cluster above with a large amount of variation and one cluster with a low amount of variation. Coordinate with your group to try to get a number of different clusters chosen per group.\n",
    "\n",
    "Use the code below to plot the PHATE embedding of that cluster and color it by the likelihood values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the cluster names\n",
    "print(metadata['cluster'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate these cells to test multiple clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========\n",
    "# Pick a cluster to analyze\n",
    "curr_cluster = \"\"\n",
    "# =========\n",
    "\n",
    "# Take a subset of the data\n",
    "curr_subset = metadata['cluster'] == curr_cluster\n",
    "curr_data = data_pca.loc[curr_subset]\n",
    "curr_metadata = metadata.loc[curr_subset]\n",
    "curr_data_phate = phate.PHATE(verbose=0).fit_transform(curr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scprep.plot.scatter2d(curr_data_phate, \n",
    "                      c=curr_metadata['chd_likelihood'], \n",
    "                      cmap=meld.get_meld_cmap(), vmin=0, vmax=1,\n",
    "                      ticks=False, figsize=(4,4),\n",
    "                      title='{} ({} cells)'.format(curr_cluster, curr_data_phate.shape[0]), \n",
    "                      legend=False, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "What do you notice about the relationship between standard deviation of the likelihood and the amount of heterogeneity seen in the PHATE embedding? What does this suggest?\n",
    "\n",
    "This concludes the workshop version of the notebook, but to see how vertex frequency clustering interacts with MELD to identify subpopulations of cells, please consult [the full tutorial on GitHub](https://github.com/KrishnaswamyLab/MELD)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
